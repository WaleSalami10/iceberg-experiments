{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed1d249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'org.apache.iceberg:iceberg-spark3-runtime:0.11.0', 'spark.sql.extensions': 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', 'spark.sql.catalog.spark_catalog': 'org.apache.iceberg.spark.SparkSessionCatalog', 'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.sql.catalog.local': 'org.apache.iceberg.spark.SparkCatalog', 'spark.sql.catalog.local.type': 'hadoop', 'spark.sql.catalog.local.warehouse': 's3://vasveena-test-demo/iceberg/catalog/tables/'}, 'proxyUser': 'user_vasveena', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1631106604406_0002</td><td>spark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-54.ec2.internal:20888/proxy/application_1631106604406_0002/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-46-72.ec2.internal:8042/node/containerlogs/container_1631106604406_0002_01_000001/livy\" >Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars.packages\":\"org.apache.iceberg:iceberg-spark3-runtime:0.11.0\",\n",
    "             \"spark.sql.extensions\":\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "             \"spark.sql.catalog.spark_catalog\":\"org.apache.iceberg.spark.SparkSessionCatalog\",\n",
    "             \"spark.sql.catalog.spark_catalog.type\":\"hive\",\n",
    "             \"spark.sql.catalog.local\":\"org.apache.iceberg.spark.SparkCatalog\",\n",
    "             \"spark.sql.catalog.local.type\":\"hadoop\",\n",
    "             \"spark.sql.catalog.local.warehouse\":\"s3://vasveena-test-demo/iceberg/catalog/tables/\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2015e6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad14904948c24f488239cd3aca44deb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1631106604406_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-54.ec2.internal:20888/proxy/application_1631106604406_0003/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-39-251.ec2.internal:8042/node/containerlogs/container_1631106604406_0003_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: String = 3.0.1-amzn-0\n"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a42c461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19b22277dcb4c75a85ca50a1e10d4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_df: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 5 more fields]\n",
      "res2: Long = 2700000000\n"
     ]
    }
   ],
   "source": [
    "val input_df = spark.read.parquet(\"s3://vasveena-test-demo/tmp/hudi-perf/input/\")\n",
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99734692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f5c2a5a927431e901eec3c49f4edf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_df.createOrReplaceTempView(\"inputdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9390a061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e8e063c82542fcadf24a137f72b1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions._\n",
      "input_df2: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|4000000|    3|4000000|[E]|6e505939-f5fd-4ab...|2019|2021-04-02 00:05:02|  9|      v1|    v2|2021-04-02|\n",
      "|4000001|    9|4000001|[F]|20486aca-2759-43f...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000002|   11|4000002|[G]|42962a21-a2dc-40d...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000003|    9|4000003|[H]|9841ad6d-1532-496...|2019|2021-04-02 00:05:02|  c|      v1|    v2|2021-04-02|\n",
      "|4000004|    4|4000004|[I]|ff1a855a-cced-495...|2019|2021-04-02 00:05:02|  4|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val input_df2=(input_df.withColumn(\"z\", substring(md5(concat($\"id\")),1,1))\n",
    "                       .withColumn(\"schema-v\", lit(\"v1\")).withColumn(\"data-v\", lit(\"v2\"))\n",
    "                       .withColumn(\"trade_dt\", substring($\"modified_timestamp\",1,10)))\n",
    "input_df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de28e6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d97a7cb6de34c41aed5bf41d63d8ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 5459068382856\n",
      "s3_location: String = s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_66\n",
      "duration: String = 122.388460322 seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "val s3_location=s\"s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_66\"\n",
    "\n",
    "input_df2.write.mode(\"OVERWRITE\").partitionBy(\"z\",\"schema-v\",\"data-v\",\"trade_dt\").parquet(s3_location)\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \" seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9964f066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a70e3e4c6e4eafa48f2d4fa396bd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 5625578477832\n",
      "s3_location_ndf: String = s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_normaldf66\n",
      "duration: String = 140.541020354seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "val s3_location_ndf=s\"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_normaldf66\"\n",
    "\n",
    "(input_df2.write.mode(\"OVERWRITE\")\n",
    "    .option(\"path\", s3_location_ndf)\n",
    "    .partitionBy(\"z\",\"schema-v\",\"data-v\",\"trade_dt\").format(\"parquet\")\n",
    "    .saveAsTable(\"iceberg_table_normaldf\"))\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29785c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9cadd1fb0144fea14032786b4723a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Spark table Parquet output\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|      id|month|      sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|25000034|    4|25000034|[U]|38727554-feec-472...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|25000039|    3|25000039|[Z]|d98a26e8-22bd-4a4...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|25000060|    8|25000060|[U]|fd5af7ee-0c4f-42c...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|25000089|   10|25000089|[X]|990cf695-4a30-412...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|25000105|    6|25000105|[N]|5c074610-86a9-4b8...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Reading Spark table Parquet output\")\n",
    "spark.sql(\"select * from iceberg_table_normaldf limit 5\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cff83f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641121bd6b2d43f7a4bc1a1df0133b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_df4: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val input_df4 = input_df2.withColumnRenamed(\"schema-v\", \"schema_v\").withColumnRenamed(\"data-v\", \"data_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e4bd3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a170c5711cde4a17b09e7b72c58a4614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n"
     ]
    }
   ],
   "source": [
    "input_df4.registerTempTable(\"inputdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ddcb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847a2a174a13407bba478229ed0028b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res18: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table local.db.iceberg_table_sparksqldf26\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cecae6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3452bdbed1134226b11674c7a2f83968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res19: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE local.db.iceberg_table_sparksqldf26 (id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       txt struct<key1:string>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       schema_v string,\n",
    "                                       data_v string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.target-file-size-bytes'=10485760,\n",
    "          'write.parquet.row-group-size-bytes'=10485760,\n",
    "          'write.object-storage.path'='s3://vasveena-test-hmswh/')\n",
    "PARTITIONED BY (z,schema_v,data_v,trade_dt)\n",
    "location  's3://vasveena-test-demo/iceberg/catalog/tables/db/iceberg_table_sparksqldf26'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "387f90ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e1d7cc56f94c9a812b91e6dcff3649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.spark.SparkException: Writing job aborted.\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:413)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:361)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.writeWithV2(WriteToDataSourceV2Exec.scala:273)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.OverwriteByExpressionExec.run(WriteToDataSourceV2Exec.scala:286)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:39)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:39)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:45)\n",
      "  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:230)\n",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3667)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3665)\n",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:230)\n",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:98)\n",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)\n",
      "  ... 53 elided\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 316 in stage 11.0 failed 4 times, most recent failure: Lost task 316.3 in stage 11.0 (TID 4238, ip-172-31-35-255.ec2.internal, executor 668): java.lang.IllegalStateException: Already closed files for partition: z=0/schema_v=v1/data_v=v2/trade_dt=trade_dt\n",
      "\tat org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2215)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2164)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2163)\n",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2163)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1013)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1013)\n",
      "  at scala.Option.foreach(Option.scala:407)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1013)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2395)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2344)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:815)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:382)\n",
      "  ... 80 more\n",
      "Caused by: java.lang.IllegalStateException: Already closed files for partition: z=0/schema_v=v1/data_v=v2/trade_dt=trade_dt\n",
      "  at org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "  ... 3 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "spark.sql(\"\"\"insert overwrite table local.db.iceberg_table_sparksqldf26\n",
    "             partition (z,`schema_v`,`data_v`,trade_dt) \n",
    "             select id, month, sk, txt, uuid,\n",
    "             year, modified_timestamp, z,\n",
    "             `schema_v`, `data_v`,'trade_dt'\n",
    "             from inputdf4\n",
    "             \"\"\")\n",
    "//this test is without order by. expected ERROR\n",
    "//order by clause is required to avoid error \"java.lang.IllegalStateException: Already closed files for partition: z=6/schema_v=v1/data_v=v2/trade_dt=trade_dt\"\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf5ca7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4abc88dfe8487fa2653f14252fd858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.spark.SparkException: Writing job aborted.\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:413)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:361)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.OverwritePartitionsDynamicExec.writeWithV2(WriteToDataSourceV2Exec.scala:306)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.OverwritePartitionsDynamicExec.run(WriteToDataSourceV2Exec.scala:314)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:39)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:39)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:54)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:124)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:123)\n",
      "  at org.apache.spark.sql.DataFrameWriterV2.$anonfun$runCommand$1(DataFrameWriterV2.scala:226)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "  at org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:226)\n",
      "  at org.apache.spark.sql.DataFrameWriterV2.overwritePartitions(DataFrameWriterV2.scala:216)\n",
      "  ... 53 elided\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 350 in stage 12.0 failed 4 times, most recent failure: Lost task 350.3 in stage 12.0 (TID 5365, ip-172-31-42-245.ec2.internal, executor 776): java.lang.IllegalStateException: Already closed files for partition: z=4/schema_v=v1/data_v=v2/trade_dt=2021-04-02\n",
      "\tat org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2215)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2164)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2163)\n",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2163)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1013)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1013)\n",
      "  at scala.Option.foreach(Option.scala:407)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1013)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2395)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2344)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:815)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:382)\n",
      "  ... 80 more\n",
      "Caused by: java.lang.IllegalStateException: Already closed files for partition: z=4/schema_v=v1/data_v=v2/trade_dt=2021-04-02\n",
      "  at org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "  ... 3 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "//Trying out same with iceberg writeTo. expected same ERROR\n",
    "input_df4.writeTo(\"local.db.iceberg_table_sparksqldf26\").overwritePartitions()\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7209c6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5a8f5f341e42bdabcd8507c15e111c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 6741207571881\n",
      "res30: org.apache.spark.sql.DataFrame = []\n",
      "duration: String = 1436.138836923seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "spark.sql(\"\"\"insert overwrite table local.db.iceberg_table_sparksqldf26\n",
    "             partition (z,`schema_v`,`data_v`,trade_dt) \n",
    "             select id, month, sk, txt, uuid,\n",
    "             year, modified_timestamp, z,\n",
    "             `schema_v`, `data_v`,'trade_dt'\n",
    "             from inputdf4\n",
    "             order by z,`schema_v`,`data_v`,trade_dt \"\"\") \n",
    "//expected: no error\n",
    "//order by clause is required to avoid error \"java.lang.IllegalStateException: Already closed files for partition: z=6/schema_v=v1/data_v=v2/trade_dt=trade_dt\"\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6945bb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8beecc065be48eba97cca08af2ab45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res36: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE local.db.iceberg_table_sparksqldf22 (id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       txt struct<key1:string>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       schema_v string,\n",
    "                                       data_v string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.target-file-size-bytes'=10485760,\n",
    "          'write.parquet.row-group-size-bytes'=10485760,\n",
    "          'write.object-storage.path'='s3://vasveena-test-hmswh/')\n",
    "PARTITIONED BY (z,schema_v,data_v,trade_dt)\n",
    "location  's3://vasveena-test-demo/iceberg/catalog/tables/db/iceberg_table_sparksqldf22'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78c263c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d59fa18b0549b593099f1f97fdd204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 8531439011479\n",
      "duration: String = 147.244818994seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "//Trying out same with iceberg writeTo. expected same ERROR\n",
    "input_df4.sortWithinPartitions(\"z\",\"schema_v\",\"data_v\",\"trade_dt\").writeTo(\"local.db.iceberg_table_sparksqldf22\").overwritePartitions()\n",
    "//expected no error \n",
    "//local sort instead of global sort \n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6270ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
