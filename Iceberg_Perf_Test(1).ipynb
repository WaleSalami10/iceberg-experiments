{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4b3251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T08:25:33.979039Z",
     "start_time": "2021-04-23T08:25:33.958862Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: line 2: /home/emr-notebook/.sparkmagic/config.json: No such file or directory\n",
      "/home/emr-notebook/.sparkmagic/config.json: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "echo '{\"kernel_python_credentials\" : {\"url\": \"http://ip-172-31-32-142.ec2.internal:8998/\"}, \"session_configs\": \n",
    "{\"executorMemory\": \"2g\",\"executorCores\": 2,\"numExecutors\":4}}' > ~/.sparkmagic/config.json\n",
    "less ~/.sparkmagic/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f899a1d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T19:09:37.406184Z",
     "start_time": "2021-04-23T19:08:54.050673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'org.apache.iceberg:iceberg-spark3-runtime:0.11.0', 'spark.sql.extensions': 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', 'spark.sql.catalog.spark_catalog': 'org.apache.iceberg.spark.SparkSessionCatalog', 'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.sql.catalog.local': 'org.apache.iceberg.spark.SparkCatalog', 'spark.sql.catalog.local.type': 'hadoop', 'spark.sql.catalog.local.warehouse': 's3://vasveena-test-demo/iceberg/catalog/tables/'}, 'proxyUser': 'user_vasveena', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1619633879001_0008</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-172-31-41-141.ec2.internal:20888/proxy/application_1619633879001_0008/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-35-252.ec2.internal:8042/node/containerlogs/container_1619633879001_0008_01_000001/livy\" >Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars.packages\":\"org.apache.iceberg:iceberg-spark3-runtime:0.11.0\",\n",
    "             \"spark.sql.extensions\":\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "             \"spark.sql.catalog.spark_catalog\":\"org.apache.iceberg.spark.SparkSessionCatalog\",\n",
    "             \"spark.sql.catalog.spark_catalog.type\":\"hive\",\n",
    "             \"spark.sql.catalog.local\":\"org.apache.iceberg.spark.SparkCatalog\",\n",
    "             \"spark.sql.catalog.local.type\":\"hadoop\",\n",
    "             \"spark.sql.catalog.local.warehouse\":\"s3://vasveena-test-demo/iceberg/catalog/tables/\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be08424",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T19:21:10.763285Z",
     "start_time": "2021-04-23T19:21:10.736193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4a15f1a3ee435d992fe8462f65129c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1619633879001_0009</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-41-141.ec2.internal:20888/proxy/application_1619633879001_0009/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-35-252.ec2.internal:8042/node/containerlogs/container_1619633879001_0009_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'3.0.1-amzn-0'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "config = {\n",
    "    \"table_name\": \"example_iceberg_perf_test_4\"\n",
    "}\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814fc8c",
   "metadata": {},
   "source": [
    "## Read the Data\n",
    "\n",
    "Let's read our 100M record dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df91c2e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T19:21:24.488735Z",
     "start_time": "2021-04-23T19:21:13.216814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54422fdd970b4260a4ad912f67eb170d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000"
     ]
    }
   ],
   "source": [
    "input_df = spark.read.parquet(\"s3://neilawstmp2/tmp/hudi-perf/input/\")\n",
    "input_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d004b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd70eeb3b2e46e48968d7222cf1a265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_df.createOrReplaceTempView(\"inputdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429943a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T19:21:24.722810Z",
     "start_time": "2021-04-23T19:21:24.491341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398ec1ad5ee74e1c8b33c5b02cbc5fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: struct (nullable = true)\n",
      " |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      "\n",
      "25"
     ]
    }
   ],
   "source": [
    "input_df.printSchema()\n",
    "input_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f99b6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T19:21:29.077280Z",
     "start_time": "2021-04-23T19:21:28.465299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://neilawstmp2/tmp/hudi-perf/input/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b43fb",
   "metadata": {},
   "source": [
    "## Write Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "382e9b00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T00:40:02.013292Z",
     "start_time": "2021-04-23T00:39:56.757355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c56151596d44239ccd859fec3238d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|4000000|    3|4000000|[E]|6e505939-f5fd-4ab...|2019|2021-04-02 00:05:02|  9|      v1|    v2|2021-04-02|\n",
      "|4000001|    9|4000001|[F]|20486aca-2759-43f...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000002|   11|4000002|[G]|42962a21-a2dc-40d...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000003|    9|4000003|[H]|9841ad6d-1532-496...|2019|2021-04-02 00:05:02|  c|      v1|    v2|2021-04-02|\n",
      "|4000004|    4|4000004|[I]|ff1a855a-cced-495...|2019|2021-04-02 00:05:02|  4|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "input_df=input_df.withColumn(\"z\",expr(\"substr(md5(concat(id)),1,1)\"))\n",
    "input_df=input_df.withColumn('schema-v',lit('v1'))\n",
    "input_df=input_df.withColumn('data-v',lit('v2'))\n",
    "input_df=input_df.withColumn('trade_dt',expr(\"substr(modified_timestamp,1,10)\"))\n",
    "input_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "affd6669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T00:47:09.715101Z",
     "start_time": "2021-04-23T00:45:02.044622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487b771f4a95444d830efb589198b82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#s3_location=f\"s3://neilawstmp2/tmp/parquet-perf/catalog/{config['table_name']}\"\n",
    "\n",
    "s3_location=f\"s3://vasveena-test-demo/temp6/parquet-perf/catalog/{config['table_name']}\"\n",
    "\n",
    "partition_cols=['z','schema-v','data-v','trade_dt']\n",
    "input_df.write.mode(\"OVERWRITE\").partitionBy(partition_cols).parquet(s3_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6b9e9",
   "metadata": {},
   "source": [
    "### Bulk Insert in Iceberg\n",
    "\n",
    "Now let's write the same out in Iceberg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cd50477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T19:23:07.451857Z",
     "start_time": "2021-04-23T19:21:33.855747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f38f5991a854bada1dcb83c6ba3ba7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed in : 97.53534412384033"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#s3_location=f\"s3://neilawstmp2/iceberg/tables/catalog/{config['table_name']}\"\n",
    "#data_location=\"s3://neilawstmp2/iceberg/tables/data/\"\n",
    "\n",
    "s3_location=f\"s3://vasveena-test-demo/iceberg/tables/catalog/{config['table_name']}\"\n",
    "data_location=\"s3://vasveena-test-demo/iceberg/tables/data2/\"\n",
    "\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "partition_cols=['year']\n",
    "input_df.write.mode(\"OVERWRITE\")\\\n",
    ".option('path',s3_location) \\\n",
    ".option('write.object-storage.enabled','true')\\\n",
    ".option('write.object-storage.path',data_location)\\\n",
    ".partitionBy(partition_cols).format(\"iceberg\")\\\n",
    ".saveAsTable(config[\"table_name\"])\n",
    "\n",
    "print(f\"Executed in : {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaf884",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c82a81e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T19:18:37.100238Z",
     "start_time": "2021-04-23T19:18:35.839296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d5ae5ec13b4a2b9b0ad8719bd1ae20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------+-----------+\n",
      "|database|tableName                  |isTemporary|\n",
      "+--------+---------------------------+-----------+\n",
      "|default |example_iceberg_perf_test_1|false      |\n",
      "|default |example_iceberg_perf_test_4|false      |\n",
      "|default |table_sample_iceberg4      |false      |\n",
      "|default |table_sample_iceberg66     |false      |\n",
      "|        |inputdf                    |true       |\n",
      "+--------+---------------------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3ff08bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T19:20:16.051038Z",
     "start_time": "2021-04-23T19:18:42.529314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE example_iceberg_perf_test\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "310cd281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T09:08:24.179389Z",
     "start_time": "2021-04-23T09:08:02.869771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|100000000|\n",
      "+---------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from example_iceberg_perf_test LIMIT 10\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a337d4cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T09:08:57.585269Z",
     "start_time": "2021-04-23T09:08:57.355746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Failed to execute SHOW CREATE TABLE against table `default`.`example_iceberg_perf_test`, which is created by Hive and uses the following unsupported serde configuration\n",
      " SERDE: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe INPUTFORMAT: org.apache.hadoop.mapred.FileInputFormat OUTPUTFORMAT: org.apache.hadoop.mapred.FileOutputFormat;\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 649, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 134, in deco\n",
      "    raise_from(converted)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "pyspark.sql.utils.AnalysisException: Failed to execute SHOW CREATE TABLE against table `default`.`example_iceberg_perf_test`, which is created by Hive and uses the following unsupported serde configuration\n",
      " SERDE: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe INPUTFORMAT: org.apache.hadoop.mapred.FileInputFormat OUTPUTFORMAT: org.apache.hadoop.mapred.FileOutputFormat;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CREATE TABLE example_iceberg_perf_test\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34bad4a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:53:52.464178Z",
     "start_time": "2021-04-22T19:53:51.891851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://neilawstmp2/tmp/iceberg-perf/catalog/example_iceberg_perf_test/data/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f325970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:53:55.789077Z",
     "start_time": "2021-04-22T19:53:55.220483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE data/\n",
      "                           PRE metadata/\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://neilawstmp2/tmp/iceberg-perf/catalog/example_iceberg_perf_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1021081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:53:59.119177Z",
     "start_time": "2021-04-22T19:53:58.539213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-22 19:52:55  129152558 00000-25-3dfc4b64-a6cb-4662-9686-566b1c9bf8da-00001.parquet\n",
      "2021-04-22 19:52:55  129150847 00001-26-b9a44a7d-0dfd-47fd-85a3-38538b20355e-00001.parquet\n",
      "2021-04-22 19:52:58  129155524 00002-27-e22f0c01-ff59-4a2e-9cd1-4bc6d375df27-00001.parquet\n",
      "2021-04-22 19:52:58  129154578 00003-28-eb503339-2986-4c29-b0e1-4487e3cf9555-00001.parquet\n",
      "2021-04-22 19:52:57  126367557 00004-29-18675f0e-73ea-40a7-bfa4-813822b82656-00001.parquet\n",
      "2021-04-22 19:52:50  101098090 00005-30-1dca0f83-361b-4fa0-89ea-690e6d57e46c-00001.parquet\n",
      "2021-04-22 19:52:50  101097695 00006-31-2026de01-a8ef-486a-a8f7-fc9f2dd321e2-00001.parquet\n",
      "2021-04-22 19:52:50  101097023 00007-32-fe028d46-dd64-4fce-94ba-bff641328b42-00001.parquet\n",
      "2021-04-22 19:52:49  101094489 00008-33-35923c84-1670-4701-8930-10e9d295fdec-00001.parquet\n",
      "2021-04-22 19:52:41   75824046 00009-34-ef522a10-51d2-4abc-b445-c5080d916cf8-00001.parquet\n",
      "2021-04-22 19:52:42   75823811 00010-35-95a999c5-f679-4bf6-bd57-58b1c1a0b582-00001.parquet\n",
      "2021-04-22 19:52:42   75822641 00011-36-c2684292-6116-47cb-8340-f8ea5a8796d7-00001.parquet\n",
      "2021-04-22 19:52:43   75821766 00012-37-7f05b61a-d440-4e3f-bfb4-455c125b9521-00001.parquet\n",
      "2021-04-22 19:52:43   75821687 00013-38-7d97c42d-91e7-42bb-8473-ce18bad14398-00001.parquet\n",
      "2021-04-22 19:52:44   75821855 00014-39-aa76de33-85b7-4b4d-8b04-b1da8d81c413-00001.parquet\n",
      "2021-04-22 19:52:44   75821153 00015-40-e11bf42d-4133-4389-9096-c095ba90425f-00001.parquet\n",
      "2021-04-22 19:53:20  126370694 00016-41-aef6b868-818e-4107-bf51-2e7a6abc3356-00001.parquet\n",
      "2021-04-22 19:53:15  101096163 00017-42-0304af0e-1c70-41af-94a2-c9ca3288b1a4-00001.parquet\n",
      "2021-04-22 19:53:14  101096123 00018-43-405c0d81-45be-45c0-8a13-fc2d25bb1e3a-00001.parquet\n",
      "2021-04-22 19:53:15  101096260 00019-44-cae8c5f0-9294-4d97-a639-09133eafcbcf-00001.parquet\n",
      "2021-04-22 19:53:15  101093053 00020-45-1384775c-4b13-4e5b-a3c2-d9ba2b29f95e-00001.parquet\n",
      "2021-04-22 19:53:20  120806858 00021-46-6c4e97e4-3506-4b6c-b158-1aba870cdfdb-00001.parquet\n",
      "2021-04-22 19:53:17  101097617 00022-47-f5a0fbd1-5b97-4e32-a203-49d6ac366487-00001.parquet\n",
      "2021-04-22 19:53:21  101095844 00023-48-af7b65b5-346d-4b1a-a4dc-a8d241996d14-00001.parquet\n",
      "2021-04-22 19:53:18   95529793 00024-49-16b03c7f-e646-4cf4-9b49-1e2e7f7aa72b-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://neilawstmp2/tmp/iceberg-perf/catalog/example_iceberg_perf_test/data/year=2019/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b901cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:02:25.368681Z",
     "start_time": "2021-04-22T20:02:25.136615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eff73834cc943f980cfde5b25612060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE default.table_sample_iceberg66\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41266502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:02:42.638643Z",
     "start_time": "2021-04-22T20:02:41.904000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2faeb4053f2403296aa391e20509c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE table_sample_iceberg6666 (id bigint,data string,category string,ts timestamp,year string) \\\n",
    "USING iceberg \\\n",
    "OPTIONS ( \\\n",
    "    'write.object-storage.enabled'=true,  \\\n",
    "    'write.object-storage.path'='s3://vasveena-test-demo/iceberg/tables/data6666/') \\\n",
    "PARTITIONED BY (year) \\\n",
    "LOCATION 's3://vasveena-test-demo/temp5/parquet-perf/catalog/example_iceberg_perf_test_6666'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba60b69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:11:32.904903Z",
     "start_time": "2021-04-22T20:11:27.643696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411dfbb3e414405aa774b63608b74b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------+-----------+\n",
      "|database|tableName                  |isTemporary|\n",
      "+--------+---------------------------+-----------+\n",
      "|default |example_iceberg_perf_test_1|false      |\n",
      "|default |example_iceberg_perf_test_4|false      |\n",
      "|default |table_sample_iceberg4      |false      |\n",
      "|default |table_sample_iceberg6666   |false      |\n",
      "|        |inputdf                    |true       |\n",
      "+--------+---------------------------+-----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: struct (nullable = true)\n",
      " |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema-v: string (nullable = false)\n",
      " |-- data-v: string (nullable = false)\n",
      " |-- trade_dt: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(10,False)\n",
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f95624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa527980a0b44e4b95f7b086bb1cc216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"insert overwrite table table_sample_iceberg6666 partition (year) select id,month,uuid,modified_timestamp,year from inputdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a8f47d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:12:17.079109Z",
     "start_time": "2021-04-22T20:12:16.846168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "<console>:23: error: object iceberg is not a member of package org.apache\n",
      "       import org.apache.iceberg.spark.IcebergSpark\n",
      "                         ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.iceberg.spark.IcebergSpark\n",
    "import org.apache.spark.sql.types.DataTypes\n",
    "\n",
    "//IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket16\", DataTypes.LongType, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2259983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T20:05:26.650528Z",
     "start_time": "2021-04-22T20:05:03.332099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o84.sql.\n",
      ": org.apache.spark.SparkException: Writing job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:413)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:361)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:253)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:259)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:45)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:230)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3667)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3665)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:230)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:98)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)\n",
      "\tat sun.reflect.GeneratedMethodAccessor326.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 150, ip-172-31-37-196.us-west-2.compute.internal, executor 30): java.lang.IllegalStateException: Already closed files for partition: id_bucket=15\n",
      "\tat org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2215)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2164)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2163)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2163)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1013)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1013)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1013)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2395)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2344)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:815)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:382)\n",
      "\t... 37 more\n",
      "Caused by: java.lang.IllegalStateException: Already closed files for partition: id_bucket=15\n",
      "\tat org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 649, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 128, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o84.sql.\n",
      ": org.apache.spark.SparkException: Writing job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:413)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:361)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:253)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:259)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:45)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:230)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3667)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3665)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:230)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:98)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)\n",
      "\tat sun.reflect.GeneratedMethodAccessor326.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 150, ip-172-31-37-196.us-west-2.compute.internal, executor 30): java.lang.IllegalStateException: Already closed files for partition: id_bucket=15\n",
      "\tat org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2215)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2164)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2163)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2163)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1013)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1013)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1013)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2395)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2344)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:815)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:382)\n",
      "\t... 37 more\n",
      "Caused by: java.lang.IllegalStateException: Already closed files for partition: id_bucket=15\n",
      "\tat org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO table_sample_iceberg \\\n",
    "SELECT id, month, year, modified_timestamp FROM example_iceberg_perf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40c7060",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparksql_magic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f376abef5d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sparksql_magic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-57>\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparksql_magic'"
     ]
    }
   ],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959ba2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
