{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65686a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 's3://vasveena-test-demo/jars/iceberg-spark3-runtime-0.11.0.jar', 'spark.driver.extraClassPath': '/home/hadoop/iceberg-spark3-runtime-0.11.0.jar', 'spark.executor.extraClassPath': '/home/hadoop/iceberg-spark3-runtime-0.11.0.jar', 'spark.sql.extensions': 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', 'spark.sql.catalog.spark_catalog': 'org.apache.iceberg.spark.SparkSessionCatalog', 'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.sql.catalog.local': 'org.apache.iceberg.spark.SparkCatalog', 'spark.sql.catalog.local.type': 'hadoop', 'spark.sql.catalog.local.warehouse': 's3://vasveena-test-demo/iceberg/catalog/tables/'}, 'proxyUser': 'user_vasveena', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"s3://vasveena-test-demo/jars/iceberg-spark3-runtime-0.11.0.jar\",\n",
    "             \"spark.driver.extraClassPath\":\"/home/hadoop/iceberg-spark3-runtime-0.11.0.jar\",\n",
    "             \"spark.executor.extraClassPath\":\"/home/hadoop/iceberg-spark3-runtime-0.11.0.jar\",\n",
    "             \"spark.sql.extensions\":\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "             \"spark.sql.catalog.spark_catalog\":\"org.apache.iceberg.spark.SparkSessionCatalog\",\n",
    "             \"spark.sql.catalog.spark_catalog.type\":\"hive\",\n",
    "             \"spark.sql.catalog.local\":\"org.apache.iceberg.spark.SparkCatalog\",\n",
    "             \"spark.sql.catalog.local.type\":\"hadoop\",\n",
    "             \"spark.sql.catalog.local.warehouse\":\"s3://vasveena-test-demo/iceberg/catalog/tables/\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594194eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135c296d87d245b6ae1ded7f859c8d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 10 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "Exception in thread \"main\" java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.amazon.ws.emr.hadoop.fs.EmrFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2637)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3324)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3356)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3407)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3375)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:486)\n",
      "\tat org.apache.spark.deploy.DependencyUtils$.resolveGlobPath(DependencyUtils.scala:191)\n",
      "\tat org.apache.spark.deploy.DependencyUtils$.$anonfun$resolveGlobPaths$2(DependencyUtils.scala:147)\n",
      "\tat org.apache.spark.deploy.DependencyUtils$.$anonfun$resolveGlobPaths$2$adapted(DependencyUtils.scala:145)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.DependencyUtils$.resolveGlobPaths(DependencyUtils.scala:145)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$4(SparkSubmit.scala:363)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:363)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1015)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1024)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.lang.ClassNotFoundException: Class com.amazon.ws.emr.hadoop.fs.EmrFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2541)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2635)\n",
      "\t... 27 more\n",
      "\n",
      "YARN Diagnostics: \n",
      "spark-submit start failed.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97bd656b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6e7b170084418fbd361a8220e14e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_df: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 5 more fields]\n",
      "res2: Long = 2700000000\n"
     ]
    }
   ],
   "source": [
    "val input_df = spark.read.parquet(\"s3://vasveena-test-demo/tmp/hudi-perf/input/\")\n",
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223bd2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1e6976b7024180849ab8df2f231fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_df.createOrReplaceTempView(\"inputdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9073af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec653ea8e4f344899e903afebd238f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions._\n",
      "input_df2: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|4000000|    3|4000000|[E]|6e505939-f5fd-4ab...|2019|2021-04-02 00:05:02|  9|      v1|    v2|2021-04-02|\n",
      "|4000001|    9|4000001|[F]|20486aca-2759-43f...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000002|   11|4000002|[G]|42962a21-a2dc-40d...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000003|    9|4000003|[H]|9841ad6d-1532-496...|2019|2021-04-02 00:05:02|  c|      v1|    v2|2021-04-02|\n",
      "|4000004|    4|4000004|[I]|ff1a855a-cced-495...|2019|2021-04-02 00:05:02|  4|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val input_df2=(input_df.withColumn(\"z\", substring(md5(concat($\"id\")),1,1))\n",
    "                       .withColumn(\"schema-v\", lit(\"v1\")).withColumn(\"data-v\", lit(\"v2\"))\n",
    "                       .withColumn(\"trade_dt\", substring($\"modified_timestamp\",1,10)))\n",
    "input_df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f03826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107bdc362d4a41b78b525a24ffef8b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_df4: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val input_df4 = input_df2.withColumnRenamed(\"schema-v\", \"schema_v\").withColumnRenamed(\"data-v\", \"data_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84167d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table local.db.iceberg_table_sparksqldf22buck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450e3932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d71a00f86984b22936e43e21a58d85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res7: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE local.db.iceberg_table_sparksqldf22buck (id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       txt struct<key1:string>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       schema_v string,\n",
    "                                       data_v string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-hmswh/')\n",
    "PARTITIONED BY (z,schema_v,data_v,trade_dt)\n",
    "CLUSTERED BY (uuid) INTO 10 BUCKETS\n",
    "location  's3://vasveena-test-demo/iceberg/catalog/tables/db/iceberg_table_sparksqldf22buck'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69e45ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706a8ecb45b04f069fd718ec7afacb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.spark.SparkException: Writing job aborted.\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:413)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:361)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.OverwritePartitionsDynamicExec.writeWithV2(WriteToDataSourceV2Exec.scala:306)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.OverwritePartitionsDynamicExec.run(WriteToDataSourceV2Exec.scala:314)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:39)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:39)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:54)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:124)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:123)\n",
      "  at org.apache.spark.sql.DataFrameWriterV2.$anonfun$runCommand$1(DataFrameWriterV2.scala:226)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:107)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:104)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:227)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:131)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "  at org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:226)\n",
      "  at org.apache.spark.sql.DataFrameWriterV2.overwritePartitions(DataFrameWriterV2.scala:216)\n",
      "  ... 53 elided\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 151 in stage 5.0 failed 4 times, most recent failure: Lost task 151.3 in stage 5.0 (TID 1107, ip-172-31-33-225.ec2.internal, executor 74): java.lang.IllegalStateException: Already closed files for partition: z=0/schema_v=v1/data_v=v2/trade_dt=2021-04-02/uuid_bucket=7\n",
      "\tat org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2215)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2164)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2163)\n",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2163)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1013)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1013)\n",
      "  at scala.Option.foreach(Option.scala:407)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1013)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2395)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2344)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2333)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:815)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:382)\n",
      "  ... 80 more\n",
      "Caused by: java.lang.IllegalStateException: Already closed files for partition: z=0/schema_v=v1/data_v=v2/trade_dt=2021-04-02/uuid_bucket=7\n",
      "  at org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
      "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "  ... 3 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "input_df4.sortWithinPartitions(\"z\",\"schema_v\",\"data_v\",\"trade_dt\").writeTo(\"local.db.iceberg_table_sparksqldf22buck\").overwritePartitions()\n",
    "//expected - no error \n",
    "//local sort instead of global sort \n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fade5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180a74359d744400a80017ef896ba048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "<console>:27: error: object iceberg is not a member of package org.apache\n",
      "       import org.apache.iceberg.spark.IcebergSpark\n",
      "                         ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// This block was run before adding the JAR to driver/exec classpath and Spark lib path on EMR master node  \n",
    "\n",
    "import org.apache.iceberg.spark.IcebergSpark\n",
    "import org.apache.spark.sql.types.DataTypes\n",
    "\n",
    "IcebergSpark.registerBucketUDF(sparkSession, \"iceberg_bucket10\", DataTypes.StringType, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a47f40",
   "metadata": {},
   "source": [
    "<b> Added iceberg-spark3-runtime-0.11.0.jar to /usr/lib/spark/jars,  restarted the kernel and re-ran the blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "986d8043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae197340bb34c48a7f2e6d827432080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res38: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE local.db.iceberg_table_sparksqldf24buck (id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       txt struct<key1:string>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       schema_v string,\n",
    "                                       data_v string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-hmswh/')\n",
    "PARTITIONED BY (bucket(10, uuid),z,schema_v,data_v,trade_dt)\n",
    "location  's3://vasveena-test-demo/iceberg/catalog/tables/db/iceberg_table_sparksqldf24buck'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c0941c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411e60c058324e749957e2feab2be216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.iceberg.spark.IcebergSpark\n",
      "import org.apache.spark.sql.types.DataTypes\n"
     ]
    }
   ],
   "source": [
    "import org.apache.iceberg.spark.IcebergSpark\n",
    "import org.apache.spark.sql.types.DataTypes\n",
    "\n",
    "IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket10\", DataTypes.StringType, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "851f1bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eefc2bfc10c418ca617fca13f97bd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 5615478160436\n",
      "duration: String = 561.422428453seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "input_df4.sortWithinPartitions(expr(\"iceberg_bucket10(uuid)\"),col(\"z\"),col(\"schema_v\"),col(\"data_v\"),col(\"trade_dt\")).writeTo(\"local.db.iceberg_table_sparksqldf24buck\").append()\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d63c87db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9f918e72004ade8364ee934fe4274a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|      id|month|      sk|txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|61000029|   11|61000029|[Z]|c45ef45d-547f-4cf...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|61000202|   11|61000202|[Q]|6e7f7d83-aeb7-469...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|61000206|    8|61000206|[U]|0032c34f-29f0-49e...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|61000323|    6|61000323|[H]|9c8bc057-4921-4ac...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|61000559|    7|61000559|[J]|c1a60f24-80ca-4c0...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|61000957|   10|61000957|[R]|f2b7ec42-6c08-491...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|61000966|    9|61000966|[A]|7bc7e168-3b8d-403...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|61001399|   10|61001399|[R]|22977260-101c-402...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|61001602|   12|61001602|[M]|11b4f257-f916-4f4...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|61001778|   10|61001778|[G]|eb24b25f-bca9-484...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from local.db.iceberg_table_sparksqldf24buck limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de4aa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e0caf2253246559ceac26df825287f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|2700000000|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from local.db.iceberg_table_sparksqldf24buck limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57713e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
